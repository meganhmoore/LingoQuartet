{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2ecee81-4f15-4ca3-a948-e03d33591176",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d4/vbphhpzx0_585fnvq9gjq5z00000gn/T/ipykernel_29182/2427569972.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ba870b2-1867-44b6-8cb2-d71a81d36226",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/meganmoore/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/meganmoore/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/meganmoore/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e03bd733-7e74-4e3c-8a4d-79e853f6dc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': 'ascii', 'confidence': 1.0, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/metadata_w_2020articles.json\", 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e6b2abb-c093-4e47-9e30-d2f8fc2f42e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('../data/metadata_w_2020articles.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2efa7bf2-f732-4c56-ae8f-3277b9a50b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.T.reset_index().rename(columns={'index':'uuid'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7047d21e-52df-4f9c-9545-702c1961b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lowercase text\n",
    "df['title'] = df['title'].str.lower()\n",
    "df['article_text'] = df['article_text'].str.lower()\n",
    "\n",
    "#remove certain characters from title\n",
    "df['title'] = df['title'].apply(lambda x: re.sub(r'[\\n\\t\\r]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbcc1404-c1e6-4699-b914-cc8f8479e94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate text and title and reshorten\n",
    "df['title_text'] = (df['title'] + ' ' +  df['article_text']).apply(lambda x: x[:512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd896f81-0007-4a59-b9e4-424da381898d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking that they were shortened on the right dimension\n",
    "test_val = df.loc[df.loc[:, 'uuid'] == 'bcbc6bb2-406e-11ee-a96e-33dec8f414a2', :]\n",
    "len(test_val['title_text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "504bb1d2-b1a5-403b-8e9f-4a0af3e71726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jackie's lemmatizer\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_tokens = word_tokenize(text)  \n",
    "    lemmatized_text = [lemmatizer.lemmatize(token) for token in word_tokens]\n",
    "    \n",
    "    return ' '.join(lemmatized_text)\n",
    "\n",
    "    # Efficiency concern? we tokenize to lemmatize, and then re-join into a string. \n",
    "    # Embeddings should be created in the loop?\n",
    "lemmatized_df = df.copy()\n",
    "lemmatized_df['title_text'] = lemmatized_df['title_text'].apply(lemmatize)\n",
    "lemmatized_df['article_text'] = lemmatized_df['article_text'].apply(lemmatize)\n",
    "lemmatized_df['title'] = lemmatized_df['title'].apply(lemmatize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f8fadbf-ab43-4df3-a005-9c508d42714c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out lemmatized version to save time\n",
    "lemmatized_df.to_csv('../data/metadata_w_2020articles_lemmatized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f224ab72-b96f-4672-917a-67bbcfd28189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out cleaned version without lemmatization in case that made things weird\n",
    "df.to_csv('../data/metadata_w_2020articles_cleaned.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
